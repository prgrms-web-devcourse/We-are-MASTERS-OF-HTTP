# 5주차 내용 퀴즈

## 9장 : 웹 로봇

###
- 크롤러가 방문을 시작하는 URL들의 초기 집합을 루트 집합이라고 한다 (O/X)
- 로봇에 웹을 크롤링 할 때, 루프나 순환에 빠지지 않도록 조심해야 한다. (O/X)
- 웹 크롤러가 방문한 곳을 관리하기 위해 사용하는 기법 중 하나는 "트리와 해시 테이블"이다 (O/X)
- 웹 로봇이 URL을 표준 형식으로 정규화 하는 이유는 다른 URL과 같은 리소스를 가리키고 있는 것들을 제거하기 위해서이다. (O/X)
- 로봇은 요청헤더 "User-Agent"로 서버에게 로봇의 이름을 알려준다. (O/X)
- 로봇의 접근을 제어하는 정보를 저장하는 파일의 이름은 보통 robots.txt라고 부른다. (O/X)
- robots.txt에 Disallow로 어떤 URl 경로가 금지되는지 명시할 수 있다. (O/X)
- 네이버의 웹 크롤러의 이름은 Yeti, 구글 웹 크롤러의 이름은 Googlebot 이다 (O/X)
<details>
<summary>답안</summary>
<div markdown="1">

전부 O

</div>
</details>

<br>

## 10장: HTTP/2.0

### 

HTTP/2.0은 서버와 클라이언트 사이의 TCP 커넥션 위에서 동작한다.  (O/X)
HTTP/2.0 요청과 응답은 길이가 정의된 "프레임"에 담긴다. (O/X)
HTTP/1.1은 헤더 필드의 이름과 값을 바이너리로 인코딩한다. (O/X)
HTTP/2.0은 헤더 필드로 어떠한 문자열을 사용할 수 있다. (O/X)
HTTP/2.0은 클라이언트와 서버 사이에 긴 커넥션을 가지고 있어 개인정보가 유출될 가능성이 있다. (O/X)

<details>
<summary>답안</summary>
<div markdown="1">

O
O
X
O
O

</div>
</details>

<br>
